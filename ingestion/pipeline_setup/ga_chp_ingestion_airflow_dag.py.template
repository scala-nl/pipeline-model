import datetime
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator

args = { 'owner': 'airflow',
         'start_date': START_DATE_AS_PY_CODE,
         'retries': 16,
         'retry_delay': datetime.timedelta(minutes=30) }

dag = DAG(dag_id='ga_chp_ingestion_pipeline',
          default_args=args,
          schedule_interval='0 12 * * *')

# Do not remove the extra space at the end (the one after 'runconnector.sh')
task_1_run_connector_cmd_parts = [
    'DAY_OF_DATA_CAPTURE={{ ds }}',
    'docker run --rm',
    '-v /opt/secrets:/opt/secrets:ro',
    '-v /opt/ga_chp:/opt/ga_chp:ro',
    '-e DAY_OF_DATA_CAPTURE',
    '-e KEY_FILE_LOCATION',
    '-e VIEW_ID',
    '-e ENVIRONMENT_TYPE',
    '-e MORPHL_SERVER_IP_ADDRESS',
    '-e MORPHL_CASSANDRA_USERNAME',
    '-e MORPHL_CASSANDRA_KEYSPACE',
    '-e MORPHL_CASSANDRA_PASSWORD',
    'pythoncontainer',
    'bash /opt/ga_chp/ingestion/connector/runconnector.sh ']
task_1_run_connector_cmd = ' '.join(task_1_run_connector_cmd_parts)

task_1_run_connector = BashOperator(
    task_id='task_1_run_connector',
    bash_command=task_1_run_connector_cmd,
    dag=dag)

# Do not remove the extra space at the end (the one after 'ga_chp_preflight_check_before_prediction_pipeline.sh')
task_2_preflight_check_before_prediction_pipeline = BashOperator(
    task_id='task_2_preflight_check_before_prediction_pipeline',
    bash_command='bash /opt/ga_chp/ingestion/preflight_check/ga_chp_preflight_check_before_prediction_pipeline.sh ',
    dag=dag)

task_2_preflight_check_before_prediction_pipeline.set_upstream(task_1_run_connector)

